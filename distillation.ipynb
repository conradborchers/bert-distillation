{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fe9bf-cc63-4356-9055-18baf71d1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92155d6-da74-4f41-803b-92cface355d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714ac9f-dab4-4c22-adc4-4388453a261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = #... Read in your data with columns representing an ID, input, and grading (human-coded data); see README for data reference\n",
    "df = df[['Transaction_Id', 'Input', 'Open_response_score_human_truth']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0e1ad-c247-40fd-90ec-d35cb077d720",
   "metadata": {},
   "source": [
    "## Starting with Building Cultural Competence Predicted (Largest Human-Coded Sample in Mixed Condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f198a-c2c9-4884-939f-c171ce492258",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PROMPT_SYS = \"\"\"\n",
    "You are a tutor responding to a middle school student in a way that makes them feel noticed, valued, and cared for, \n",
    "while also showing a genuine effort to understand and appreciate the student's cultural background.\n",
    "\"\"\"\n",
    "\n",
    "POS_PROMPT_USER = \"\"\"\n",
    "Please draft 10 positive, culturally sensitive responses that help build a connection with the student. \n",
    "Only give the responses and do not generate anything else. Do not enumerate the responses, just \n",
    "separate them one per line.\n",
    " \n",
    "Below are examples and explanations for what constitutes culturally sensitive responses:\n",
    "\n",
    "-Culturally sensitive responses help the student feel noticed and cared for by making an effort to understand and \n",
    "value a student's cultural background. Sample responses include: \n",
    "\n",
    "\"Thank her for correcting your pronunciation and ask more about her culture\" and \n",
    "\"Did you know that the early Aztecs in Mexico discovered Pythagoras theorem beforehand to create sun temples?\"; and \n",
    "\"it is okay to be forgetful sometimes why don't you tell me a bit about yourself after we do our work for the day.\"\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "NEG_PROMPT_SYS = \"\"\"\n",
    "You are a tutor responding to a middle school student in a way that does not make them feel noticed, valued, or cared for, \n",
    "and does not show any effort to understand or appreciate the student's cultural background.\n",
    "\"\"\"\n",
    "\n",
    "NEG_PROMPT_USER = \"\"\"\n",
    "Please draft 10 neutral or culturally unaware responses that fail to build a meaningful connection with the student. \n",
    "Only give the responses and do not generate anything else. Do not enumerate the responses, just \n",
    "separate them one per line.\n",
    "\n",
    "Below are examples and explanations for what constitutes culturally unaware responses:\n",
    "\n",
    "-Culturally unaware responses do not show an effort to understand or value a student's cultural background. Sample responses include:\n",
    "\n",
    "\"Thank you for correcting me. Please let me know if I make a mistake again.\" and \n",
    "\"Hi, Marcelo. You seem quiet today. Let's focus on the lesson, and we can chat afterward if there's time.\"\n",
    "\n",
    "Response Start ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe83ab6-fd66-4eef-ae17-477ef6b27461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "def read_api_token(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.readline().strip()\n",
    "\n",
    "def get_prompt(response_type):\n",
    "    if response_type.lower() == \"positive\":\n",
    "        return POS_PROMPT_SYS, POS_PROMPT_USER\n",
    "    elif response_type.lower() == \"negative\":\n",
    "        return NEG_PROMPT_SYS, NEG_PROMPT_USER\n",
    "    else:\n",
    "        raise ValueError(\"Invalid response type. Choose either 'positive' or 'negative'.\")\n",
    "\n",
    "def generate_responses(response_type, temperature=0.3):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=read_api_token('token-cb.txt'),\n",
    "    )\n",
    "    prompt_sys, prompt_user = get_prompt(response_type)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_sys},\n",
    "                {\"role\": \"user\", \"content\": prompt_user},\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content.strip().split(\"\\n\")        \n",
    "    except Exception as e:\n",
    "        return [f\"An error occurred: {str(e)}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21053d90-2d99-477c-8a3b-a7fbd8dc50a5",
   "metadata": {},
   "source": [
    "## Generation with a specific temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61507c61-bff7-46e2-9a88-3c15d5f24f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "TEMPERATURE = 0.7\n",
    "for _ in tqdm(range(50)):\n",
    "    time.sleep(1)\n",
    "    positive_responses = generate_responses(\"positive\", temperature=TEMPERATURE)\n",
    "    for response in positive_responses:\n",
    "        data.append({\"text\": response, \"category\": \"positive\", \"temperature\": TEMPERATURE})\n",
    "   \n",
    "   # Generate negative responses\n",
    "    negative_responses = generate_responses(\"negative\", temperature=TEMPERATURE)\n",
    "    for response in negative_responses:\n",
    "        data.append({\"text\": response, \"category\": \"negative\", \"temperature\": TEMPERATURE})#\n",
    "\n",
    " Create a DataFrame\n",
    "df_tmp = pd.DataFrame(data)\n",
    "df_tmp.to_csv('your-output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f3f65-5590-4ae1-a431-b7308929d309",
   "metadata": {},
   "source": [
    "## Validation through GPT consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb70e9f-4d52-4cde-ad67-80ad375238dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SYSTEM = \"\"\"\n",
    "You are a tutor evaluator. Please score the following tutor response to a tutor training scenario involving a middle school student as follows:\n",
    "-if the tutor’s response helps the student feel noticed and cared for by making an effort to understand and value a student's cultural background, score with a 1. Sample responses scoring a 1 include: \"Thank her for correcting your pronunciation and ask more about her culture\" and \"Did you know that the early Aztecs in Mexico discovered Pythagoras theorem beforehand to create sun temples?\"; and \"it is okay to be forgetful sometimes why don't you tell me a bit about yourself after we do our work for the day.\"\n",
    "-if the tutor’s response does not show a building of cultural competence, shown by a lack of care for understanding a student's background, score with a 0. Sample responses scoring a 0 include: \"Thank you for correcting me. Please correct me again in the future if it comes up\"; \"Hi Marcelo, I noticed that you are a bit quiet today. Can we have a conversation? I want to spend some time introducing myself.\"\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_USER_START = \"\"\"\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "PROMPT_USER_END = \"\"\"\n",
    "--- Response End. Given the earlier response, please return a JSON string following the format, {\\\"Rationale\\\": \\\"your reasoning here\\\", \\\"Score\\\":0/1}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1e9ae-329c-435d-871e-08254860360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synth = pd.read_csv('your-output.csv').dropna()  \n",
    "\n",
    "df_synth['category'] = df_synth.category.map(lambda x: float(1) if x=='positive' else float(0))\n",
    "df_synthtrain = df_synth\\\n",
    "    .rename(columns={'text': 'X', 'category': 'y'})\\\n",
    "    [['X', 'y']]\\\n",
    "    .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c6328-c406-41c6-8a9b-7f2770a09efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_validation(text):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=read_api_token('cb-token.txt'),\n",
    "    )\n",
    "    prompt_user = PROMPT_USER_START + str(text) + PROMPT_USER_END\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PROMPT_SYSTEM},\n",
    "                {\"role\": \"user\", \"content\": prompt_user},\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content       \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fa5e5-50fc-4a05-a885-f1e34eea3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading\n",
    "responses = []\n",
    "for generated_text in tqdm(df_soft.Input):\n",
    "    time.sleep(0.2)\n",
    "    tmp = generate_responses_validation(generated_text)\n",
    "    responses.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac78c9-8bf5-47af-b64a-e82b37c27363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def parse_gpt_json(s: str):\n",
    "    try:\n",
    "        s = s.replace('```json', '')\n",
    "        s = s.replace('```', '')\n",
    "        j = ast.literal_eval(s)\n",
    "        return j.get('Score')\n",
    "    except e:\n",
    "        print(f'Error parsing JSON: {e}, returning an empty string.')\n",
    "        return ''\n",
    "\n",
    "df_synth['gpt_grade'] = [parse_gpt_json(r) for r in resp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02bb82-2816-430f-a383-0fe473c5f2b5",
   "metadata": {},
   "source": [
    "### Evaluation of consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29263c91-a163-417d-a3ef-88343781a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Filter out only 0 and 1 labels\n",
    "binary_df = sampled_df[(sampled_df['category'].isin([0, 1])) & (sampled_df['gpt_grade'].isin([0, 1]))]\n",
    "\n",
    "# Calculate Cohen's kappa\n",
    "cohen_kappa = cohen_kappa_score(binary_df['category'], binary_df['gpt_grade'])\n",
    "print(\"Cohen's kappa:\", cohen_kappa)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(binary_df['category'], binary_df['gpt_grade'])\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(binary_df['category'], binary_df['gpt_grade'])\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(binary_df['category'], binary_df['gpt_grade'])\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(binary_df['category'], binary_df['gpt_grade'])\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c2e31-2241-44e0-9c45-de0797110bc7",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38b965-fd80-4f83-9db1-075877e64fa3",
   "metadata": {},
   "source": [
    "### In this example, a separate transfer set is defined (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2e9d6-a2e4-4c44-b99e-ea181c04628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groundtruth = df[(df['Type']=='predicted') & (df['Level (Lesson)']=='Building Cultural Competence')]\\\n",
    "    [['Input', 'Open_response_score_human_truth']]\\\n",
    "    .copy()\\\n",
    "    .rename(columns={'Input': 'X', 'Open_response_score_human_truth': 'y'})\\\n",
    "    .dropna()\n",
    "\n",
    "df_transfer = df[(df['Type']=='explained') & (df['Level (Lesson)']=='Building Cultural Competence')]\\\n",
    "    [['Input', 'Open_response_score_human_truth']]\\\n",
    "    .copy()\\\n",
    "    .rename(columns={'Input': 'X', 'Open_response_score_human_truth': 'y'})\\\n",
    "    .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab410a3-7565-44c9-8b60-d6c26ae801f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_LIST = [] # Storing results, which is later the foundation for plots. Global scope as baseline is re-referneced across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482375d-6a37-4b29-9e44-708d36831b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Split df_groundtruth into a training set and a test set (U_df) for evaluation\n",
    "train_df, U_df = train_test_split(df_groundtruth, test_size=2/3, random_state=42)\n",
    "\n",
    "# Add transfer set (explained) to U_df (evaluation test set)\n",
    "U_df = pd.concat([U_df, df_transfer]).copy()\n",
    "\n",
    "print(f'Number of human training set observations: {train_df.shape[0]}')\n",
    "print(f'Number of evaluation set observations: {U_df.shape[0]}')\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a custom PyTorch Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = TextDataset(train_df['X'].tolist(), train_df['y'].tolist(), tokenizer)\n",
    "test_dataset = TextDataset(U_df['X'].tolist(), U_df['y'].tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "\n",
    "# Function to evaluate the model and calculate AUC\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred_probs.extend(probs)\n",
    "\n",
    "     # Calculate AUC\n",
    "    auc_estimate = roc_auc_score(y_true, y_pred_probs)\n",
    "\n",
    "    # Bootstrap to calculate 95% CI\n",
    "    bootstrapped_aucs = []\n",
    "    for _ in range(1000):  # 1000 bootstrap samples\n",
    "        indices = resample(np.arange(len(y_true)), replace=True)\n",
    "        y_true_boot = np.array(y_true)[indices]\n",
    "        y_pred_probs_boot = np.array(y_pred_probs)[indices]\n",
    "        try:\n",
    "            bootstrapped_aucs.append(roc_auc_score(y_true_boot, y_pred_probs_boot))\n",
    "        except ValueError:\n",
    "            continue  # Handle case where AUC cannot be computed for a sample\n",
    "\n",
    "    lower_bound = np.percentile(bootstrapped_aucs, 2.5) # 95% CIs\n",
    "    upper_bound = np.percentile(bootstrapped_aucs, 97.5) # 95% CIs\n",
    "\n",
    "    return auc_estimate, lower_bound, upper_bound\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_auc = 0  # AUC ranges from 0 to 1, so the initial value is the lowest possible\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "counter = 0\n",
    "\n",
    "# Train the BERT model with early stopping\n",
    "for epoch in range(50):  # Set a high number of epochs initially\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Training loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    val_auc, lower, upper = evaluate(model, test_loader)\n",
    "    print(f\"Validation AUC: {val_auc}, [{lower}, {upper}]\")\n",
    "    EVAL_LIST.append((f'Epoch_{epoch}', val_auc, lower, upper))\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        counter = 0\n",
    "        # Save the best model for later experiments\n",
    "        torch.save(model.state_dict(), 'your-model.pt')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa33e6-e9c4-4d78-960c-9e7af5600eef",
   "metadata": {},
   "source": [
    "### After no improvement on human data is observed, synthetic data is added in increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e401780-223e-484d-ab0e-62acbc1b125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_eval(synth_f, THE_TEMPERATURE, remove_consistent=False, outf='plot.pdf'):\n",
    "    df_synth = pd.read_csv(synth_f).dropna() # 1000 exactly. Great\n",
    "    \n",
    "    df_synth['category'] = df_synth.category.map(lambda x: float(1) if x=='positive' else float(0))\n",
    "\n",
    "    if remove_consistent:\n",
    "        df_synth = df_synth[df_synth['category'] == df_synth['gpt_grade']].copy()\n",
    "        print(f'retaining {df_synth.shape[0]} observations')\n",
    "    \n",
    "    df_synthtrain = df_synth\\\n",
    "        .rename(columns={'text': 'X', 'category': 'y'})\\\n",
    "        [['X', 'y']]\\\n",
    "        .copy()\n",
    "    \n",
    "    with open('eval_list-final.p', 'rb') as f:\n",
    "        eval_list = pickle.load(f) \n",
    "    \n",
    "    # Before continuing training, read in \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set up the optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    model.load_state_dict(torch.load('your-model.pt')) # Load baseline model trained on human-coded data\n",
    "    \n",
    "    baseline_auc = evaluate(model, test_loader)\n",
    "    print(f\"Baseline AUC: {baseline_auc}\")\n",
    "\n",
    "    # Shuffle df_synthtrain to randomize the order\n",
    "    df_synthtrain_shuffled = df_synthtrain.sample(frac=1, random_state=42)\n",
    "    synth_texts = df_synthtrain_shuffled['X'].tolist()\n",
    "    synth_labels = df_synthtrain_shuffled['y'].tolist()\n",
    "    \n",
    "    # List to store AUC scores for each increment\n",
    "    auc_scores = []\n",
    "    \n",
    "    # Incrementally introduce data from df_synthtrain for training\n",
    "    N = 25  # Number of data points to add from df_synthtrain in each increment\n",
    "    for i in tqdm(list(range(0, 250, N))):\n",
    "        # Select the next N data points from df_synthtrain\n",
    "        current_texts = synth_texts[i:i + N]\n",
    "        current_labels = synth_labels[i:i + N]\n",
    "    \n",
    "        # Create a new DataLoader for the incrementally added data\n",
    "        new_dataset = TextDataset(current_texts, current_labels, tokenizer)\n",
    "        new_loader = DataLoader(new_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "        # Augment the training data and retrain the model\n",
    "        model.train()\n",
    "        for batch in new_loader:\n",
    "            input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            labels = batch['label'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Re-evaluate the model on the same test set (U_df) using AUC\n",
    "        auc, lower, upper = evaluate(model, test_loader)\n",
    "        auc_scores.append((auc, lower, upper))\n",
    "        eval_list.append((f'+ {i+N}', auc, lower, upper))\n",
    "        print(f\"AUC after adding {i + N} data points from df_synthtrain: {auc}\")\n",
    "\n",
    "    avg = np.mean([avg for ref, avg, _, _ in eval_list if '+' in ref])\n",
    "    print(f'Augmentation average: {avg} which is {avg-baseline_auc} above baseline')\n",
    "\n",
    "    def increment_index(s):\n",
    "        ii = int(s.replace('Epoch_', ''))\n",
    "        return s.replace(str(ii), str(ii+1))\n",
    "        \n",
    "    labels = [item[0] for item in eval_list]\n",
    "    labels = [increment_index(l) if 'Epoch' in l else l for l in labels] # fix 0-indexing of epoch...\n",
    "    means = [item[1] for item in eval_list]\n",
    "    lower_bounds = [item[2] for item in eval_list]\n",
    "    upper_bounds = [item[3] for item in eval_list]\n",
    "    \n",
    "    # Convert bounds to error ranges\n",
    "    errors = [\n",
    "        (mean - lower, upper - mean)\n",
    "        for mean, lower, upper in zip(means, lower_bounds, upper_bounds)\n",
    "    ]\n",
    "    \n",
    "    # Plot with error bars\n",
    "    plt.figure(figsize=(8, 5))  # Adjusted size for better font balance\n",
    "    plt.errorbar(\n",
    "        labels, means, yerr=np.array(errors).T, fmt='o', capsize=5, label='AUC with 95% CI'\n",
    "    )\n",
    "    plt.xlabel('Epochs and Augmentations', fontsize=11)\n",
    "    plt.ylabel('AUC Score', fontsize=10)\n",
    "    plt.axhline(y=0.7439784053156147, color='red', linestyle='--', label='Baseline AUC') # Manually setting baseline AUC based on output...\n",
    "    plt.title(f'Temperature: {THE_TEMPERATURE}', fontsize=16)\n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outf, format=\"pdf\", dpi=300)  # Save plot as PDF\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584261b-d404-4495-a3b3-d94ee5fb4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_eval('your-output.csv', 0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
